--- Starting RL Agent Training (Local) ---
Algorithm: PPO, Timesteps: 2000000
Hyperparameters: {'policy_type': 'MlpPolicy', 'total_timesteps': 2000000, 'env_name': 'OT2Env-v2', 'algorithm': 'PPO', 'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2}
C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\gymnasium\spaces\box.py:236: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\gymnasium\spaces\box.py:306: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to runs/zszzq082\PPO_1
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -4.62e+04 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 1         |
|    time_elapsed    | 21        |
|    total_timesteps | 2048      |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -5.25e+04    |
| time/                   |              |
|    fps                  | 94           |
|    iterations           | 2            |
|    time_elapsed         | 43           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0008729306 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | -0.000101    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.85e+05     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1            |
|    value_loss           | 5.81e+05     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -4.43e+04    |
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 3            |
|    time_elapsed         | 66           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 7.749279e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.00212      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.62e+05     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000189    |
|    std                  | 0.995        |
|    value_loss           | 9.6e+05      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1e+03        |
|    ep_rew_mean          | -4.58e+04    |
| time/                   |              |
|    fps                  | 91           |
|    iterations           | 4            |
|    time_elapsed         | 89           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0007817339 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.000216     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18e+05     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1            |
|    value_loss           | 2.39e+05     |
------------------------------------------
Traceback (most recent call last):
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 84, in main
    model.learn(
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 222, in step
    return self.step_wait()
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\ot2_gym_wrapper_2.py", line 64, in step
    observation = self.sim.run([scaled_action])
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 253, in run
    return self.get_states()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 304, in get_states
    raw_joint_states = p.getJointStates(robotId, [0, 1, 2])
pybullet.error: Not connected to physics server.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 112, in <module>
    main(args)
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 96, in main
    env.close()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\ot2_gym_wrapper_2.py", line 94, in close
    self.sim.close()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 439, in close
    p.disconnect()
pybullet.error: Not connected to physics server.
Traceback (most recent call last):
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 84, in main
    model.learn(
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 222, in step
    return self.step_wait()
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "C:\Users\dari\anaconda3\envs\block_b\lib\site-packages\stable_baselines3\common\monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\ot2_gym_wrapper_2.py", line 64, in step
    observation = self.sim.run([scaled_action])
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 253, in run
    return self.get_states()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 304, in get_states
    raw_joint_states = p.getJointStates(robotId, [0, 1, 2])
pybullet.error: Not connected to physics server.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 112, in <module>
    main(args)
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\train_rl.py", line 96, in main
    env.close()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\ot2_gym_wrapper_2.py", line 94, in close
    self.sim.close()
  File "C:\Users\dari\Documents\GitHub\2024-25b-fai2-adsai-dariavladutu236578\robotics\sim_class.py", line 439, in close
    p.disconnect()
pybullet.error: Not connected to physics server.
